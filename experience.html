<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <title>Experience</title>
</head>
<body>
    <div class="navbar">
        <a href="index.html">Home</a>
        <a href="photography.html">Photography</a>
        <a href="experience.html">Experience</a>
    </div>

    <div class="content" id="experience">
        <h1>Experience</h1>
        <section class="experience-section">
            <h2>CyberSaint</h2>
            <p><strong>Machine Learning Engineer</strong> - May 2020 to Present</p>
            <p>Cybersaint is made up of a small yet powerful team of visionaries. Originally focused on compliance, we've diversified to stay ahead in the evolving cybersecurity landscape. As a curious engineer, I am remarkably grateful to work with a team that is as nimble, adventurous, curious, and pioneering as the one I'm currently part of. I continue to advance my skill set every day with new, innovative, and largely experimental ideas. I am the sole Machine Learning Engineer and Data Scientist here at Cybersaint. Over the past few years, I have developed several innovative technologies, which I will describe below. As I've discovered here, and as I will convey in this overview, even the simplest tasks can reveal complexities beyond imagination.</p>
            
            <h3>Crosswalking</h3>
            <p>In cybersecurity, crosswalking is the act of mapping one security control framework to another for the purpose of aligning regulation standards, tracking and syncing control performance, and other data movement. Historically, it involves teams of experts and an impractical amount of time performing tedious comparisons. While ideas for improvement circulate occasionally, they largely remain manual processes. A good NLP approach will eat this problem for breakfast in a matter of minutes. The issue here is data. Frameworks are wildly unique and are constantly being adapted by companies into personalized, custom frameworks. This means that the probability of overfitting on the few crosswalk datasets you can get your hands on is high. The battle to stay generalized in the broader security space is challenging, as there aren’t many dedicated security datasets centered around language. Using industry standards (related controls available within popular frameworks) is a good start, but there is widespread disagreement on the accuracy of these mappings and in order to maintain a healthy training set you still need more negatively associated control pairs than positive.Some datasets exist in this regard that were born from an optimized crosswalking workflow and therefore produced intermediary language between controls, but even this is slightly misaligned as it represents a tree structure rather than a many-to-many dataset. </p>
            
            <h5>Efficient Crosswalking is Weak Interpretation</h5>
            <p>As new research arises from academia, the crosswalking algorithm evolves utilizing recent techniques. The original algorithm is currently being split into 2 smaller, more efficient algorithms meant to expand customization and large scale practicality, and one meant to come as close as possible to a universal mapping mechanism. Both of the two small algorithms employ a simple extension block that takes embeddings from a stock embedding algorithm. One compares controls in a pairwise fashion with a relatedness score output, and the other outputs a modified embedding and uses a FAISS indexing process to search using cosine similarity, otherwise known as a basic semantic search. Due to the data issue mentioned previously, the base embedding algorithm was intentionally frozen. The block extension is designed as an interpretation of the embedding, capturing only relevant information from the original training dataset that is present within the weights. The block is sized such that it learns the patterns of the base embedding to determine a similarity metric, but is not complex enough to overfit on the minimal or misaligned data. Other regularization techniques are used, but after running experiments this solution worked the best at finding matches. The purpose is to dissect only similarity artifacts surrounding language relevant to the cybersecurity control language used to train this block. This method is particularly useful for applications where each customer may prefer a different interpretation of which controls map to which other controls. This small block extension allows for extremely low resource training and low model file footprint. Each of these models display advantages depending on the specific task, but they both represent efficient and, arguably more importantly, individualized crosswalking.</p>

            <h5>A Stab at a Universal Cybersecurity Mapping Mechanism</h5>
            <p>The third algorithm takes a step back and takes a first principles approach to the gaps in cybersecurity. Starting with control mappings, we can treat these like a large graph network, where each node represents a single control. Recent work with graph neural network infrastructure makes it possible to now treat crosswalking like an insertion into this graph. Given different frameworks, we can group controls and classify entire frameworks. Using this as a foundation, we can create a heterogeneous graph representation of not only controls, but CVEs, threat feeds, MITRE TTPs, or any other cybersecurity indicators that contain language. As the network density increases, it improves its ability to perform few-shot learning, an important step for adding a classification task for a new data type. While still in development, this model has incredible potential for both automation as well as creating a threat landscape aware warning system.</p>

            <h3>Historical Event Modeling</h3>
            <p>Having access to historical data is invaluable when making future decisions about risk. In the past, companies have often relied on experienced opinions from employees or consultants to predict cyber threat trends and determine fund allocation for cybersecurity budgets. Extrapolating event data trends drives more confidence in projected likelihood and impact expectations, more so when the data can be cut such that the resulting subset represents your company’s size and industry. While remarkably useful, this is a predictable outcome when exposed to a dataset of this nature. Some particularly challenging yet interesting outlets come when interpolating, because like all great datasets, this one looks like swiss cheese. Utilizing statistical methods, filling gaps reveals several trends, particularly within financial loss. Exploring further, I was able to accurately predict when a company was attempting to cover up a costly event and even developed a metric to gauge the propensity to report losses at all. </p>
            
            <h5>Likelihood Modeling</h5>
            <p>Another interesting avenue within this type of data exploration was modeling likelihood. Given historical data from similarly sized and placed companies, how many events can you expect over the next year? A quick poll from an inverse poisson log normal wont hold back any predictions. Feeding this and loss distributions into a FAIR monte carlo model will return your average loss expectancy over the next year. Pack all this data up and divide it into a few buckets and not only can we drive decisions based on data rather than opinion, but we can do it with a few clicks and basic knowledge about your company size and sector, transforming a largely unknown field into a plug and play, user friendly mechanism. </p>

            <h3>Risk Quantification</h3>
            <p>The concept of risk is rather abstract. As the field matures, scoring systems become more refined, more data driven, and more actionable. This is the motivation behind our risk quantification projects. The most notable of which is a model co-developed between myself and engineers from Booz Allen Hamilton. As the name suggests, CyberInsights is meant to provide valuable risk insights into a variety of scenarios determined by an encompassing questionnaire. The basis of the model simulates access and impact using an enterprise graph and random variables initialized by a Bayesian belief network molded by the questionnaire inputs. Using a monte carlo style output and ties into MITRE Att&ck, a variety of scores and an annualized loss can assist in important decision making.</p>

            <h3>Partnerships</h3>
            <p>CyberSaint has partnered with a few companies, but specifically to AI our most prominent has been our recent relationship with IBM WatsonX. Accelerated into their program to offer our solution, we also work closely with a team within IBM on a special concept involving generative AI and risk reporting.</p>

            <h3>Patents</h3>
            <p>Involving work done here at CyberSaint, I am referenced on 2 patents currently in the process of being approved. We are also in the process of filing multiple others, all in the topic of AI in Cybersecurity.</p>        
        </section>

        <section class="education-section">
            <h2>Education</h2>
            <p><strong>Rensselaer Polytechnic Institute:</strong> Bachelor’s and Master’s Degrees in Computer Science, Graduated Spring 2020. GPA: 3.84</p>
            <h5><strong>Thesis: </strong>Estimation of Animal Orientation and Fiducial Mark Location</h5>
            <p>During graduate school at RPI, I was tasked with orienting images for the purpose of improving animal identification algorithms for <a href="https://www.wildme.org/" target="_blank">Wild Me</a>. Wild Me is a non-profit organization leading efforts to improve the animal tagging and monitoring process to track populations and migrational movement. My contribution oriented a set of animals in order to test the influence on identification accuracy. Given an animal photograph, my software warped the image such that each input image adheres to a standard input homography and passed the photograph down the identification pipeline. The basis of my algorithm was a theory of over-parameterization. Estimating orientation angle from the target rotation has numerous solutions, from as simple as outputting a theta inference to applying quaternion mathematics. I chose a redundant method of outputting the sine and cosine parts of the rotation angle for two main reasons. The first is that we get two unique estimates increasing confidence and giving an uncertainty score derived from the difference, and using trigonometry components gives a continuously differentiable loss function.</p>
        </section>
    </div>
</body>
</html>
